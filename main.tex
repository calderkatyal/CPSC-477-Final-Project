\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024

\PassOptionsToPackage{numbers}{natbib}
% ready for submission
\usepackage[preprint]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{calc}
\usepackage{algorithm}
\usepackage{algpseudocode}

\title{CSPC 477 Final Project Proposal}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\makeatletter
\renewcommand{\@noticestring}{}
\makeatother
\author{%
  Calder Katyal, Jess Yatvitskiy\\
  Yale University\\
  New Haven, CT 06511 \\
  \texttt{calder.katyal@yale.edu, jess.yatvitskiy@yale.edu} \\
}
\begin{document}
\maketitle
\vspace{-2em}
\section{Project Overview}
\label{project_overview}

The goal of this project is to develop a scalable backend for natural language email search. The proposed system leverages state-of-the-art technologies to build a hybrid search and ranking platform that integrates keyword-based and semantic retrieval methods. We will use the Enron dataset, which contains approximately 500,000 emails generated by employees of the Enron Corporation.
\section{Description of Related Works}
\label{related_works}
Traditional keyword-based search methods, such as BM25 \citep{bm25}, retrieve documents based on term frequency but lack semantic understanding. Dense retrieval models like Sentence-BERT \citep{sentencebert} improve search by leveraging embeddings for semantic similarity. Text-to-SQL models, including Seq2SQL \citep{seq2sql} and SQLNet \citep{sqlnet}, translate natural language queries into structured database queries for more precise retrieval. Hybrid approaches, such as Reciprocal Rank Fusion (RRF) \citep{rrf}, combine lexical and semantic search to enhance results. Our approach integrates many of these methods to build an efficient and scalable natural language email search system.
\section{Proposed Approaches}

\textbf{Data Ingestion \& Storage.} Emails are subjected to text cleaning (using tools such as SpaCy and NLTK) to remove signatures, stopwords, and extraneous content, while key metadata is extracted. The cleaned metadata is stored in PostgreSQL with appropriate indexing and partitioning, and high-quality embeddings are precomputed using an open-source dense retrieval model (e.g., BGE or E5-Large). These embeddings are stored via FAISS for efficient retrieval.

\textbf{Query Expansion.} Upon receiving a query, a paraphrasing model (such as Flan-T5-Large) generates several variants. Each variant is processed in parallel to capture diverse linguistic expressions.

\textbf{Hybrid Search.} Query variants are processed using two search methods. A BM25-based keyword search is performed on Elasticsearch over the indexed metadata, and a FAISS-based semantic search is conducted by converting each variant into an embedding (using the email embedding model) and retrieving emails based on cosine similarity. Candidate results from both methods are merged using Reciprocal Rank Fusion, with duplicates removed and scores normalized using Min-Max scaling.

\textbf{Weighted Scoring. } The merged candidates are scored between [0, 1] using a weighted combination of BM25 and FAISS similarity scores, with additional boosts for matches in critical metadata fields.


\section{Evaluation Metrics}
\label{metrics}

\textbf{Weighted Kendallâ€™s W for Similar Queries. } We generate $K$ similar queries and then use a weighted Kendall's W metric to measure similarity between the corresponding email rankings. 

\textbf{Weighted MSE. } We generate $K$ similar queries and then calculate weighted MSE to measure similarity between the corresponding email relevance scores. 

\textbf{Human Evaluation. } We generate queries and evaluate rankings as a sanity check.

\bibliographystyle{plainnat} 

\newpage
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}